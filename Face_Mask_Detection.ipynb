{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5b340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de9b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"C:\\Users\\ASUS\\OneDrive - Royal University of Phnom Penh\\Ai S1\\Lab1\\data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc5465ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9301 images belonging to 2 classes.\n",
      "Found 2324 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Preprocessing and augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,          # normalize\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2      # 20% validation\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',      # since mask/no-mask is binary\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    'data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "627182f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9301 images belonging to 2 classes.\n",
      "Found 2324 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    \"data\",\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    \"data\",\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "base = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224,224,3))\n",
    "base.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61f962c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 733ms/step - accuracy: 0.9404 - loss: 0.1591 - val_accuracy: 0.6601 - val_loss: 1.8037\n",
      "Epoch 2/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 704ms/step - accuracy: 0.9788 - loss: 0.0633 - val_accuracy: 0.6579 - val_loss: 1.9929\n",
      "Epoch 3/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 701ms/step - accuracy: 0.9817 - loss: 0.0536 - val_accuracy: 0.6588 - val_loss: 2.2814\n",
      "Epoch 4/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 702ms/step - accuracy: 0.9840 - loss: 0.0454 - val_accuracy: 0.6601 - val_loss: 2.3197\n",
      "Epoch 5/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 700ms/step - accuracy: 0.9852 - loss: 0.0447 - val_accuracy: 0.6605 - val_loss: 2.4925\n",
      "Epoch 6/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 691ms/step - accuracy: 0.9848 - loss: 0.0444 - val_accuracy: 0.6601 - val_loss: 2.4587\n",
      "Epoch 7/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 592ms/step - accuracy: 0.9854 - loss: 0.0437 - val_accuracy: 0.6609 - val_loss: 2.3350\n",
      "Epoch 8/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 584ms/step - accuracy: 0.9849 - loss: 0.0429 - val_accuracy: 0.6609 - val_loss: 2.3893\n",
      "Epoch 9/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 580ms/step - accuracy: 0.9842 - loss: 0.0430 - val_accuracy: 0.6592 - val_loss: 2.5210\n",
      "Epoch 10/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 581ms/step - accuracy: 0.9879 - loss: 0.0374 - val_accuracy: 0.6618 - val_loss: 2.6154\n",
      "Epoch 11/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 580ms/step - accuracy: 0.9857 - loss: 0.0403 - val_accuracy: 0.6588 - val_loss: 2.7178\n",
      "Epoch 12/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 713ms/step - accuracy: 0.9871 - loss: 0.0371 - val_accuracy: 0.6618 - val_loss: 2.3454\n",
      "Epoch 13/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 615ms/step - accuracy: 0.9887 - loss: 0.0348 - val_accuracy: 0.6575 - val_loss: 2.8678\n",
      "Epoch 14/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 592ms/step - accuracy: 0.9853 - loss: 0.0383 - val_accuracy: 0.6601 - val_loss: 2.6725\n",
      "Epoch 15/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 591ms/step - accuracy: 0.9881 - loss: 0.0377 - val_accuracy: 0.6575 - val_loss: 2.7961\n",
      "Epoch 16/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 605ms/step - accuracy: 0.9870 - loss: 0.0380 - val_accuracy: 0.6588 - val_loss: 2.8168\n",
      "Epoch 17/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 596ms/step - accuracy: 0.9867 - loss: 0.0397 - val_accuracy: 0.6592 - val_loss: 2.8823\n",
      "Epoch 18/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 596ms/step - accuracy: 0.9865 - loss: 0.0371 - val_accuracy: 0.6605 - val_loss: 2.8026\n",
      "Epoch 19/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 602ms/step - accuracy: 0.9888 - loss: 0.0342 - val_accuracy: 0.6601 - val_loss: 2.8340\n",
      "Epoch 20/20\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 596ms/step - accuracy: 0.9865 - loss: 0.0379 - val_accuracy: 0.6596 - val_loss: 2.8512\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c75f3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "418c3b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 224, 224, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63a925ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"mask_detector_model.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73a620c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(r\"C:\\Users\\ASUS\\OneDrive - Royal University of Phnom Penh\\Ai S1\\Lab1\\mask_detector_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c06f2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 390 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C713B05B20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 390 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C713B05B20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value: 3.4271987e-05\n",
      "Mask Off\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your trained model\n",
    "model_path = r\"C:\\Users\\ASUS\\OneDrive - Royal University of Phnom Penh\\Ai S1\\Lab1\\mask_detector_model.h5\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Test image path (use raw string r\"\" or forward slashes)\n",
    "img_path = r\"C:\\Users\\ASUS\\OneDrive - Royal University of Phnom Penh\\Ai S1\\Lab1\\test_img4.jpg\"\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread(img_path)\n",
    "if img is None:\n",
    "    print(\"Image not found! Check the path.\")\n",
    "else:\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, (224, 224))\n",
    "    img_resized = img_resized.astype(\"float32\") / 255.0  # normalize\n",
    "    img_resized = np.expand_dims(img_resized, axis=0)\n",
    "\n",
    "    pred = model.predict(img_resized, verbose=0)[0][0]\n",
    "    print(\"Prediction value:\", pred)\n",
    "\n",
    "    if pred > 0.5:\n",
    "        print(\"Mask On\")\n",
    "    else:\n",
    "        print(\"Mask Off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0db61267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 0.99999994\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999985\n",
      "Pred: 0.9999993\n",
      "Pred: 0.9999999\n",
      "Pred: 1.0\n",
      "Pred: 0.99999994\n",
      "Pred: 0.9999999\n",
      "Pred: 0.99999994\n",
      "Pred: 0.9999998\n",
      "Pred: 0.99999964\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999989\n",
      "Pred: 0.9999999\n",
      "Pred: 0.99999976\n",
      "Pred: 0.9999998\n",
      "Pred: 1.0\n",
      "Pred: 1.0\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999994\n",
      "Pred: 0.9999995\n",
      "Pred: 0.9999979\n",
      "Pred: 0.9999999\n",
      "Pred: 0.99999976\n",
      "Pred: 0.99999875\n",
      "Pred: 0.99999946\n",
      "Pred: 0.99999934\n",
      "Pred: 0.9999997\n",
      "Pred: 0.9999927\n",
      "Pred: 0.99998987\n",
      "Pred: 0.45721328\n",
      "Pred: 0.9999997\n",
      "Pred: 0.99999976\n",
      "Pred: 0.99999934\n",
      "Pred: 0.999999\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999997\n",
      "Pred: 0.9999998\n",
      "Pred: 0.99999976\n",
      "Pred: 0.99999124\n",
      "Pred: 0.9999929\n",
      "Pred: 0.9999999\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999493\n",
      "Pred: 0.9999981\n",
      "Pred: 0.9999989\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999946\n",
      "Pred: 0.99999404\n",
      "Pred: 0.9999931\n",
      "Pred: 0.9999936\n",
      "Pred: 0.9999968\n",
      "Pred: 0.99999684\n",
      "Pred: 0.9999935\n",
      "Pred: 0.99998945\n",
      "Pred: 0.9999966\n",
      "Pred: 0.99999917\n",
      "Pred: 0.99999756\n",
      "Pred: 0.9999987\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999995\n",
      "Pred: 0.9999995\n",
      "Pred: 0.9986287\n",
      "Pred: 0.99998164\n",
      "Pred: 0.99998164\n",
      "Pred: 0.99999577\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999972\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999991\n",
      "Pred: 0.99999976\n",
      "Pred: 0.999838\n",
      "Pred: 0.9999924\n",
      "Pred: 0.99999285\n",
      "Pred: 0.99999994\n",
      "Pred: 0.9999995\n",
      "Pred: 0.99999964\n",
      "Pred: 0.9999967\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999934\n",
      "Pred: 0.9999999\n",
      "Pred: 0.99999976\n",
      "Pred: 0.9999998\n",
      "Pred: 0.99999994\n",
      "Pred: 0.9999989\n",
      "Pred: 0.9999989\n",
      "Pred: 0.9999984\n",
      "Pred: 0.9999993\n",
      "Pred: 0.9999993\n",
      "Pred: 0.9999995\n",
      "Pred: 0.9999996\n",
      "Pred: 0.99999976\n",
      "Pred: 0.9999998\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99997777\n",
      "Pred: 0.9999719\n",
      "Pred: 0.9999969\n",
      "Pred: 0.9999984\n",
      "Pred: 0.9999982\n",
      "Pred: 0.99999607\n",
      "Pred: 0.99999964\n",
      "Pred: 0.99999976\n",
      "Pred: 0.9999988\n",
      "Pred: 0.99999934\n",
      "Pred: 0.9999994\n",
      "Pred: 0.99999785\n",
      "Pred: 0.9999997\n",
      "Pred: 0.9999997\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999994\n",
      "Pred: 0.9999997\n",
      "Pred: 0.99999976\n",
      "Pred: 0.9999999\n",
      "Pred: 0.99999917\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999999\n",
      "Pred: 0.99999976\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999976\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999997\n",
      "Pred: 0.9999997\n",
      "Pred: 0.99999964\n",
      "Pred: 0.9999996\n",
      "Pred: 0.99999934\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999993\n",
      "Pred: 0.99999934\n",
      "Pred: 0.99999523\n",
      "Pred: 0.9996227\n",
      "Pred: 0.9996227\n",
      "Pred: 0.9946228\n",
      "Pred: 0.9998612\n",
      "Pred: 0.99999774\n",
      "Pred: 0.99999976\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999996\n",
      "Pred: 0.9999996\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999997\n",
      "Pred: 0.99999917\n",
      "Pred: 0.99999946\n",
      "Pred: 0.999998\n",
      "Pred: 0.99999934\n",
      "Pred: 0.999998\n",
      "Pred: 0.99999964\n",
      "Pred: 0.99999964\n",
      "Pred: 0.9999992\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999985\n",
      "Pred: 0.9999989\n",
      "Pred: 0.9999991\n",
      "Pred: 0.999998\n",
      "Pred: 0.9999993\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999996\n",
      "Pred: 0.9999996\n",
      "Pred: 0.9999995\n",
      "Pred: 0.9999994\n",
      "Pred: 0.99999917\n",
      "Pred: 0.99999845\n",
      "Pred: 0.99999946\n",
      "Pred: 0.99999905\n",
      "Pred: 0.99999934\n",
      "Pred: 0.9999996\n",
      "Pred: 0.99999934\n",
      "Pred: 0.9999992\n",
      "Pred: 0.9999992\n",
      "Pred: 0.99999934\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999994\n",
      "Pred: 0.9999997\n",
      "Pred: 0.9999997\n",
      "Pred: 0.9999998\n",
      "Pred: 0.99999964\n",
      "Pred: 0.9999992\n",
      "Pred: 0.9999997\n",
      "Pred: 0.99999976\n",
      "Pred: 0.99999964\n",
      "Pred: 0.99999964\n",
      "Pred: 0.9999995\n",
      "Pred: 0.9999997\n",
      "Pred: 0.99999964\n",
      "Pred: 0.9999999\n",
      "Pred: 0.99999976\n",
      "Pred: 0.9999996\n",
      "Pred: 0.9999999\n",
      "Pred: 0.99999964\n",
      "Pred: 0.99999976\n",
      "Pred: 0.9999998\n",
      "Pred: 0.99999976\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999994\n",
      "Pred: 0.99999976\n",
      "Pred: 0.99999976\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999994\n",
      "Pred: 0.9999996\n",
      "Pred: 0.9999997\n",
      "Pred: 0.9999993\n",
      "Pred: 0.9999992\n",
      "Pred: 0.9999982\n",
      "Pred: 0.9999997\n",
      "Pred: 0.99999994\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999993\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999964\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999992\n",
      "Pred: 0.99999964\n",
      "Pred: 0.9999997\n",
      "Pred: 0.9999995\n",
      "Pred: 0.9999996\n",
      "Pred: 0.99999994\n",
      "Pred: 0.9999992\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999999\n",
      "Pred: 0.99999976\n",
      "Pred: 0.9999998\n",
      "Pred: 0.99999994\n",
      "Pred: 0.9999993\n",
      "Pred: 0.9999981\n",
      "Pred: 0.9999996\n",
      "Pred: 0.9999875\n",
      "Pred: 0.99999064\n",
      "Pred: 0.99999905\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999998\n",
      "Pred: 0.99991405\n",
      "Pred: 0.9999991\n",
      "Pred: 0.9999997\n",
      "Pred: 0.9999998\n",
      "Pred: 0.99999964\n",
      "Pred: 0.9999999\n",
      "Pred: 0.99999946\n",
      "Pred: 0.998679\n",
      "Pred: 0.17470244\n",
      "Pred: 0.99998915\n",
      "Pred: 0.999992\n",
      "Pred: 0.99989164\n",
      "Pred: 0.99998283\n",
      "Pred: 0.9999483\n",
      "Pred: 0.99991244\n",
      "Pred: 0.9999837\n",
      "Pred: 0.99995464\n",
      "Pred: 0.999958\n",
      "Pred: 0.99995875\n",
      "Pred: 0.99997365\n",
      "Pred: 0.99995065\n",
      "Pred: 0.99997413\n",
      "Pred: 0.99993217\n",
      "Pred: 0.99996483\n",
      "Pred: 0.9999841\n",
      "Pred: 0.9999896\n",
      "Pred: 0.99998486\n",
      "Pred: 0.9999772\n",
      "Pred: 0.9999615\n",
      "Pred: 0.99998164\n",
      "Pred: 0.9999966\n",
      "Pred: 0.99997956\n",
      "Pred: 0.9999645\n",
      "Pred: 0.9999962\n",
      "Pred: 0.99999666\n",
      "Pred: 0.9999983\n",
      "Pred: 0.9999983\n",
      "Pred: 0.9996595\n",
      "Pred: 0.36187097\n",
      "Pred: 0.99438655\n",
      "Pred: 0.99922675\n",
      "Pred: 0.9999453\n",
      "Pred: 0.9998244\n",
      "Pred: 0.99999917\n",
      "Pred: 0.9999996\n",
      "Pred: 0.99999756\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999976\n",
      "Pred: 0.999999\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999998\n",
      "Pred: 0.9999998\n",
      "Pred: 0.99993086\n",
      "Pred: 0.99998534\n",
      "Pred: 0.9999989\n",
      "Pred: 0.99999946\n",
      "Pred: 0.9999989\n",
      "Pred: 0.9999993\n",
      "Pred: 0.9999982\n",
      "Pred: 0.9999954\n",
      "Pred: 0.9999978\n",
      "Pred: 0.99999964\n",
      "Pred: 0.99999845\n",
      "Pred: 0.99999917\n",
      "Pred: 0.9999992\n",
      "Pred: 0.9999997\n",
      "Pred: 0.99999756\n",
      "Pred: 0.99999934\n",
      "Pred: 0.99999326\n",
      "Pred: 0.9999985\n",
      "Pred: 0.9999992\n",
      "Pred: 0.99999964\n",
      "Pred: 0.9999982\n",
      "Pred: 0.9999969\n",
      "Pred: 0.94941765\n",
      "Pred: 0.9687338\n",
      "Pred: 0.9413151\n",
      "Pred: 0.96707207\n",
      "Pred: 0.99996686\n",
      "Pred: 0.9999736\n",
      "Pred: 0.99846023\n",
      "Pred: 0.9999976\n",
      "Pred: 0.9999974\n",
      "Pred: 0.99959147\n",
      "Pred: 0.9999988\n",
      "Pred: 0.1185844\n",
      "Pred: 0.056377936\n",
      "Pred: 0.9999991\n",
      "Pred: 0.99999976\n",
      "Pred: 0.99999946\n",
      "Pred: 0.99999964\n",
      "Pred: 0.9999994\n",
      "Pred: 0.99999964\n",
      "Pred: 0.99999934\n",
      "Pred: 0.9999996\n",
      "Pred: 0.99999976\n",
      "Pred: 0.99999964\n",
      "Pred: 0.9999995\n",
      "Pred: 0.9999993\n",
      "Pred: 0.99999964\n",
      "Pred: 0.99999917\n",
      "Pred: 0.99999917\n",
      "Pred: 0.9999995\n",
      "Pred: 0.9999999\n",
      "Pred: 0.99999934\n",
      "Pred: 0.99999994\n",
      "Pred: 0.9999994\n",
      "Pred: 0.99999976\n",
      "Pred: 0.99999976\n",
      "Pred: 0.9999992\n",
      "Pred: 0.9999995\n",
      "Pred: 0.99999905\n",
      "Pred: 0.9999997\n",
      "Pred: 0.9999997\n",
      "Pred: 0.9999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x000001C691B93EC0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\weakref.py\", line 369, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 0.99999946\n",
      "Pred: 0.99999946\n",
      "Pred: 0.9999997\n",
      "Pred: 0.99999774\n",
      "Pred: 0.999965\n",
      "Pred: 0.9952227\n",
      "Pred: 0.9997277\n",
      "Pred: 0.9999995\n",
      "Pred: 0.99999994\n",
      "Pred: 0.9999998\n",
      "Pred: 0.99999994\n",
      "Pred: 1.0\n",
      "Pred: 0.9999995\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999994\n",
      "Pred: 0.99999636\n",
      "Pred: 0.9999969\n",
      "Pred: 0.9999973\n",
      "Pred: 0.9999999\n",
      "Pred: 0.9999997\n",
      "Pred: 0.99999696\n",
      "Pred: 0.99991727\n",
      "Pred: 0.9991198\n",
      "Pred: 0.99579066\n",
      "Pred: 0.99999404\n",
      "Pred: 0.99995196\n",
      "Pred: 0.9784438\n",
      "Pred: 0.9999501\n",
      "Pred: 0.9101017\n",
      "Pred: 0.9928359\n",
      "Pred: 0.42601094\n",
      "Pred: 0.8255927\n",
      "Stopped by user\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from playsound import playsound\n",
    "import time\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Load trained model\n",
    "# -----------------------------\n",
    "model_path = r\"C:\\Users\\ASUS\\OneDrive - Royal University of Phnom Penh\\Ai S1\\Lab1\\mask_detector_model.h5\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Haar Cascade face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Alert sound\n",
    "alert_file = r\"C:\\Users\\ASUS\\OneDrive - Royal University of Phnom Penh\\Ai S1\\Lab1\\mask_alert.wav\"\n",
    "\n",
    "# Alert settings\n",
    "last_alert_time = 0\n",
    "alert_delay = 2  # seconds between alerts\n",
    "\n",
    "# Prediction threshold\n",
    "mask_threshold = 0.5  # adjust based on your model\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"Face Mask Detector\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Face Mask Detector\", 800, 600)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(60,60))\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            last_alert_time = 0  # reset timer if no faces\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "            face_resized = cv2.resize(face_rgb, (224, 224))\n",
    "            face_resized = face_resized.astype(\"float32\") / 255.0  # normalize\n",
    "            face_resized = np.expand_dims(face_resized, axis=0)\n",
    "\n",
    "            pred = model.predict(face_resized, verbose=0)[0][0]\n",
    "            print(\"Pred:\", pred)\n",
    "\n",
    "            if pred > mask_threshold:\n",
    "                label = \"Mask On\"\n",
    "                color = (0, 255, 0)\n",
    "            else:\n",
    "                label = \"Mask Off\"\n",
    "                color = (0, 0, 255)\n",
    "                # Play alert\n",
    "                current_time = time.time()\n",
    "                if current_time - last_alert_time > alert_delay:\n",
    "                    if os.path.exists(alert_file):\n",
    "                        playsound(alert_file)\n",
    "                    last_alert_time = current_time\n",
    "\n",
    "            # Draw rectangle and label\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "        cv2.imshow(\"Face Mask Detector\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped by user\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
